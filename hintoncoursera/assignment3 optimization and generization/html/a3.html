
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>a3</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-08-14"><meta name="DC.source" content="a3.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput"><span class="comment">% This version of the starter code was published on Tuesday November 8, 01:37 UTC. It is an improved version of the one that was published on November 6, 13:37 UTC.</span>

<span class="keyword">function</span> a3(wd_coefficient, n_hid, n_iters, learning_rate, momentum_multiplier, do_early_stopping, mini_batch_size)
  warning(<span class="string">'error'</span>, <span class="string">'Octave:broadcast'</span>);
  <span class="keyword">if</span> exist(<span class="string">'page_output_immediately'</span>), page_output_immediately(1); <span class="keyword">end</span>
  more <span class="string">off</span>;
  model = initial_model(n_hid);
  from_data_file = load(<span class="string">'data.mat'</span>);
  datas = from_data_file.data;
  n_training_cases = size(datas.training.inputs, 2);
  <span class="keyword">if</span> n_iters ~= 0,
    fprintf(<span class="string">'Now testing the gradient on the whole training set... '</span>);
    test_gradient(model, datas.training, wd_coefficient);
  <span class="keyword">end</span>

  <span class="comment">% optimization</span>
  theta = model_to_theta(model);
  momentum_speed = theta * 0;
  training_data_losses = [];
  validation_data_losses = [];
  <span class="keyword">if</span> do_early_stopping,
    best_so_far.theta = -1; <span class="comment">% this will be overwritten soon</span>
    best_so_far.validation_loss = inf;
    best_so_far.after_n_iters = -1;
  <span class="keyword">end</span>
  <span class="keyword">for</span> optimization_iteration_i = 1:n_iters,
    model = theta_to_model(theta);

    training_batch_start = mod((optimization_iteration_i-1) * mini_batch_size, n_training_cases)+1;
    training_batch.inputs = datas.training.inputs(:, training_batch_start : training_batch_start + mini_batch_size - 1);
    training_batch.targets = datas.training.targets(:, training_batch_start : training_batch_start + mini_batch_size - 1);
    gradient = model_to_theta(d_loss_by_d_model(model, training_batch, wd_coefficient));
    momentum_speed = momentum_speed * momentum_multiplier - gradient;
    theta = theta + momentum_speed * learning_rate;

    model = theta_to_model(theta);
    training_data_losses = [training_data_losses, loss(model, datas.training, wd_coefficient)];
    validation_data_losses = [validation_data_losses, loss(model, datas.validation, wd_coefficient)];
    <span class="keyword">if</span> do_early_stopping &amp;&amp; validation_data_losses(end) &lt; best_so_far.validation_loss,
      best_so_far.theta = theta; <span class="comment">% this will be overwritten soon</span>
      best_so_far.validation_loss = validation_data_losses(end);
      best_so_far.after_n_iters = optimization_iteration_i;
    <span class="keyword">end</span>
    <span class="keyword">if</span> mod(optimization_iteration_i, round(n_iters/10)) == 0,
      fprintf(<span class="string">'After %d optimization iterations, training data loss is %f, and validation data loss is %f\n'</span>, optimization_iteration_i, training_data_losses(end), validation_data_losses(end));
    <span class="keyword">end</span>
    <span class="keyword">if</span> optimization_iteration_i == n_iters, <span class="comment">% check gradient again, this time with more typical parameters and with a different data size</span>
      fprintf(<span class="string">'Now testing the gradient on just a mini-batch instead of the whole training set... '</span>);
      test_gradient(model, training_batch, wd_coefficient);
    <span class="keyword">end</span>
  <span class="keyword">end</span>
  <span class="keyword">if</span> do_early_stopping,
    fprintf(<span class="string">'Early stopping: validation loss was lowest after %d iterations. We chose the model that we had then.\n'</span>, best_so_far.after_n_iters);
    theta = best_so_far.theta;
  <span class="keyword">end</span>
  <span class="comment">% the optimization is finished. Now do some reporting.</span>
  model = theta_to_model(theta);
  <span class="keyword">if</span> n_iters ~= 0,
    clf;
    hold <span class="string">on</span>;
    plot(training_data_losses, <span class="string">'b'</span>);
    plot(validation_data_losses, <span class="string">'r'</span>);
    legend(<span class="string">'training'</span>, <span class="string">'validation'</span>);
    ylabel(<span class="string">'loss'</span>);
    xlabel(<span class="string">'iteration number'</span>);
    hold <span class="string">off</span>;
  <span class="keyword">end</span>
  datas2 = {datas.training, datas.validation, datas.test};
  data_names = {<span class="string">'training'</span>, <span class="string">'validation'</span>, <span class="string">'test'</span>};
  <span class="keyword">for</span> data_i = 1:3,
    data = datas2{data_i};
    data_name = data_names{data_i};
    fprintf(<span class="string">'\nThe loss on the %s data is %f\n'</span>, data_name, loss(model, data, wd_coefficient));
    <span class="keyword">if</span> wd_coefficient~=0,
      fprintf(<span class="string">'The classification loss (i.e. without weight decay) on the %s data is %f\n'</span>, data_name, loss(model, data, 0));
    <span class="keyword">end</span>
    fprintf(<span class="string">'The classification error rate on the %s data is %f\n'</span>, data_name, classification_performance(model, data));
  <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="keyword">function</span> test_gradient(model, data, wd_coefficient)
  base_theta = model_to_theta(model);
  h = 1e-2;
  correctness_threshold = 1e-5;
  analytic_gradient_struct = d_loss_by_d_model(model, data, wd_coefficient);
  <span class="keyword">if</span> size(fieldnames(analytic_gradient_struct), 1) ~= 2,
     error(<span class="string">'The object returned by function d_loss_by_d_model should have exactly two field names: .input_to_hid and .hid_to_class'</span>);
  <span class="keyword">end</span>
  <span class="keyword">if</span> any(size(analytic_gradient_struct.input_to_hid) ~= size(model.input_to_hid)),
     error(sprintf([<span class="string">'The size of .input_to_hid of the return value of d_loss_by_d_model (currently [%d, %d]) should be same as the size of model.input_to_hid (currently [%d, %d])'</span>], size(analytic_gradient_struct.input_to_hid), size(model.input_to_hid)));
  <span class="keyword">end</span>
  <span class="keyword">if</span> any(size(analytic_gradient_struct.hid_to_class) ~= size(model.hid_to_class)),
     error(sprintf([<span class="string">'The size of .hid_to_class of the return value of d_loss_by_d_model (currently [%d, %d]) should be same as the size of model.hid_to_class (currently [%d, %d])'</span>], size(analytic_gradient_struct.hid_to_class), size(model.hid_to_class)));
  <span class="keyword">end</span>
  analytic_gradient = model_to_theta(analytic_gradient_struct);
  <span class="keyword">if</span> any(isnan(analytic_gradient)) || any(isinf(analytic_gradient)),
     error(<span class="string">'Your gradient computation produced a NaN or infinity. That is an error.'</span>)
  <span class="keyword">end</span>
  <span class="comment">% We want to test the gradient not for every element of theta, because that's a lot of work. Instead, we test for only a few elements. If there's an error, this is probably enough to find that error.</span>
  <span class="comment">% We want to first test the hid_to_class gradient, because that's most likely to be correct (it's the easier one).</span>
  <span class="comment">% Let's build a list of theta indices to check. We'll check 20 elements of hid_to_class, and 80 elements of input_to_hid (it's bigger than hid_to_class).</span>
  input_to_hid_theta_size = prod(size(model.input_to_hid));
  hid_to_class_theta_size = prod(size(model.hid_to_class));
  big_prime = 1299721; <span class="comment">% 1299721 is prime and thus ensures a somewhat random-like selection of indices.</span>
  hid_to_class_indices_to_check = mod(big_prime * (1:20), hid_to_class_theta_size) + 1 + input_to_hid_theta_size;
  input_to_hid_indices_to_check = mod(big_prime * (1:80), input_to_hid_theta_size) + 1;
  indices_to_check = [hid_to_class_indices_to_check, input_to_hid_indices_to_check];
  <span class="keyword">for</span> i = 1:100,
    test_index = indices_to_check(i);
    analytic_here = analytic_gradient(test_index);
    theta_step = base_theta * 0;
    theta_step(test_index) = h;
    contribution_distances = [-4:-1, 1:4];
    contribution_weights = [1/280, -4/105, 1/5, -4/5, 4/5, -1/5, 4/105, -1/280];
    temp = 0;
    <span class="keyword">for</span> contribution_index = 1:8,
      temp = temp + loss(theta_to_model(base_theta + theta_step * contribution_distances(contribution_index)), data, wd_coefficient) * contribution_weights(contribution_index);
    <span class="keyword">end</span>
    fd_here = temp / h;
    diff = abs(analytic_here - fd_here);
    <span class="comment">% fprintf('%d %e %e %e %e\n', test_index, base_theta(test_index), diff, fd_here, analytic_here);</span>
    <span class="keyword">if</span> (diff &gt; correctness_threshold) &amp;&amp; (diff / (abs(analytic_here) + abs(fd_here)) &gt; correctness_threshold),
      part_names = {<span class="string">'input_to_hid'</span>, <span class="string">'hid_to_class'</span>};
      error(sprintf(<span class="string">'Theta element #%d (part of %s), with value %e, has finite difference gradient %e but analytic gradient %e. That looks like an error.\n'</span>, test_index, part_names{(i&lt;=20)+1}, base_theta(test_index), fd_here, analytic_here));
    <span class="keyword">end</span>
    <span class="keyword">if</span> i==20, fprintf(<span class="string">'Gradient test passed for hid_to_class. '</span>); <span class="keyword">end</span>
    <span class="keyword">if</span> i==100, fprintf(<span class="string">'Gradient test passed for input_to_hid. '</span>); <span class="keyword">end</span>
  <span class="keyword">end</span>
  fprintf(<span class="string">'Gradient test passed. That means that the gradient that your code computed is within 0.001%% of the gradient that the finite difference approximation computed, so the gradient calculation procedure is probably correct (not certainly, but probably).\n'</span>);
<span class="keyword">end</span>

<span class="keyword">function</span> ret = logistic(input)
  ret = 1 ./ (1 + exp(-input));
<span class="keyword">end</span>

<span class="keyword">function</span> ret = log_sum_exp_over_rows(a)
  <span class="comment">% This computes log(sum(exp(a), 1)) in a numerically stable way</span>
  maxs_small = max(a, [], 1);
  maxs_big = repmat(maxs_small, [size(a, 1), 1]);
  ret = log(sum(exp(a - maxs_big), 1)) + maxs_small;<span class="comment">%+log(exp(maxs_small))</span>
  <span class="comment">%=log( sum(exp(a-maxs_big),1).*exp(maxs_small) )</span>
  <span class="comment">%=log(sum(exp(a),1)./exp(max_small).*exp(max_small))</span>
<span class="keyword">end</span>

<span class="keyword">function</span> ret = loss(model, data, wd_coefficient)
  <span class="comment">% model.input_to_hid is a matrix of size &lt;number of hidden units&gt; by &lt;number of inputs i.e. 256&gt;.</span>
  <span class="comment">%It contains the weights from the input units to the hidden units.</span>
  <span class="comment">% model.hid_to_class is a matrix of size &lt;number of classes i.e. 10&gt; by &lt;number of hidden units&gt;.</span>
  <span class="comment">%It contains the weights from the hidden units to the softmax units.</span>
  <span class="comment">% data.inputs is a matrix of size &lt;number of inputs i.e. 256&gt; by &lt;number of data cases&gt;.</span>
  <span class="comment">%Each column describes a different data case.</span>
  <span class="comment">% data.targets is a matrix of size &lt;number of classes i.e. 10&gt; by &lt;number of data cases&gt;.</span>
  <span class="comment">%Each column describes a different data case. It contains a one-of-N encoding of the class, i.e. one element in every column is 1 and the others are 0.</span>

  <span class="comment">% Before we can calculate the loss, we need to calculate a variety of intermediate values,</span>
  <span class="comment">%like the state of the hidden units. This is the</span>
  <span class="comment">% forward pass, and you'll likely want to copy it into d_loss_by_d_model,</span>
  <span class="comment">%because these values are also very useful for that function.</span>
  hid_input = model.input_to_hid * data.inputs; <span class="comment">% input to the hidden units, i.e. before the logistic. size: &lt;number of hidden units&gt; by &lt;number of data cases&gt;</span>
  hid_output = logistic(hid_input); <span class="comment">% output of the hidden units, i.e. after the logistic. size: &lt;number of hidden units&gt; by &lt;number of data cases&gt;</span>
  class_input = model.hid_to_class * hid_output; <span class="comment">% input to the components of the softmax. size: &lt;number of classes, i.e. 10&gt; by &lt;number of data cases&gt;</span>

  <span class="comment">% The following three lines of code implement the softmax.</span>
  <span class="comment">% However, it's written differently from what the lectures say.</span>
  <span class="comment">% In the lectures, a softmax is described using an exponential divided by a sum of exponentials.</span>
  <span class="comment">% What we do here is exactly equivalent (you can check the math or just check it in practice), but this is more numerically stable.</span>
  <span class="comment">% "Numerically stable" means that this way, there will never be really big numbers involved.</span>
  <span class="comment">% The exponential in the lectures can lead to really big numbers, which are fine in mathematical equations, but can lead to all sorts of problems in Octave.</span>
  <span class="comment">% Octave isn't well prepared to deal with really large numbers, like the number 10 to the power 1000. Computations with such numbers get unstable, so we avoid them.</span>
  class_normalizer = log_sum_exp_over_rows(class_input);
  <span class="comment">% log(sum(exp of class_input)) is what we subtract to get properly normalized log class probabilities.</span>
  <span class="comment">%size: &lt;1&gt; by &lt;number of data cases&gt;</span>
  log_class_prob = class_input - repmat(class_normalizer, [size(class_input, 1), 1]);
  <span class="comment">% log of probability of each class. size: &lt;number of classes, i.e. 10&gt; by &lt;number of data cases&gt;</span>
  class_prob = exp(log_class_prob);
  <span class="comment">% probability of each class. Each column (i.e. each case) sums to 1. size: &lt;number of classes, i.e. 10&gt; by &lt;number of data cases&gt;</span>

  classification_loss = -mean(sum(log_class_prob .* data.targets, 1));
  <span class="comment">% select the right log class probability using that sum; then take the mean over all data cases.</span>
  wd_loss = sum(model_to_theta(model).^2)/2*wd_coefficient;
  <span class="comment">% weight decay loss. very straightforward: E = 1/2 * wd_coeffecient * theta^2</span>
  ret = classification_loss + wd_loss;
<span class="keyword">end</span>

<span class="keyword">function</span> ret = d_loss_by_d_model(model, data, wd_coefficient)
  <span class="comment">% model.input_to_hid is a matrix of size &lt;number of hidden units&gt; by &lt;number of inputs i.e. 256&gt;</span>
  <span class="comment">% model.hid_to_class is a matrix of size &lt;number of classes i.e. 10&gt; by &lt;number of hidden units&gt;</span>
  <span class="comment">% data.inputs is a matrix of size &lt;number of inputs i.e. 256&gt; by &lt;number of data cases&gt;</span>
  <span class="comment">% data.targets is a matrix of size &lt;number of classes i.e. 10&gt; by &lt;number of data cases&gt;</span>

  <span class="comment">% The returned object is supposed to be exactly like parameter &lt;model&gt;, i.e. it has fields ret.input_to_hid and ret.hid_to_class. However, the contents of those matrices are gradients (d loss by d model parameter), instead of model parameters.</span>

  <span class="comment">% This is the only function that you're expected to change. Right now, it just returns a lot of zeros, which is obviously not the correct output. Your job is to change that.</span>
  ret.input_to_hid = model.input_to_hid * 0;
  ret.hid_to_class = model.hid_to_class * 0;
<span class="keyword">end</span>

<span class="keyword">function</span> ret = model_to_theta(model)
  <span class="comment">% This function takes a model (or gradient in model form), and turns it into one long vector. See also theta_to_model.</span>
  input_to_hid_transpose = transpose(model.input_to_hid);
  hid_to_class_transpose = transpose(model.hid_to_class);
  ret = [input_to_hid_transpose(:); hid_to_class_transpose(:)];
<span class="keyword">end</span>

<span class="keyword">function</span> ret = theta_to_model(theta)
  <span class="comment">% This function takes a model (or gradient) in the form of one long vector (maybe produced by model_to_theta), and restores it to the structure format, i.e. with fields .input_to_hid and .hid_to_class, both matrices.</span>
  n_hid = size(theta, 1) / (256+10);
  ret.input_to_hid = transpose(reshape(theta(1: 256*n_hid), 256, n_hid));
  ret.hid_to_class = reshape(theta(256 * n_hid + 1 : size(theta,1)), n_hid, 10).';
<span class="keyword">end</span>

<span class="keyword">function</span> ret = initial_model(n_hid)
  n_params = (256+10) * n_hid;
  as_row_vector = cos(0:(n_params-1));
  ret = theta_to_model(as_row_vector(:) * 0.1); <span class="comment">% We don't use random initialization, for this assignment. This way, everybody will get the same results.</span>
<span class="keyword">end</span>

<span class="keyword">function</span> ret = classification_performance(model, data)
  <span class="comment">% This returns the fraction of data cases that is incorrectly classified by the model.</span>
  hid_input = model.input_to_hid * data.inputs; <span class="comment">% input to the hidden units, i.e. before the logistic. size: &lt;number of hidden units&gt; by &lt;number of data cases&gt;</span>
  hid_output = logistic(hid_input); <span class="comment">% output of the hidden units, i.e. after the logistic. size: &lt;number of hidden units&gt; by &lt;number of data cases&gt;</span>
  class_input = model.hid_to_class * hid_output; <span class="comment">% input to the components of the softmax. size: &lt;number of classes, i.e. 10&gt; by &lt;number of data cases&gt;</span>

  [dump, choices] = max(class_input); <span class="comment">% choices is integer: the chosen class, plus 1.</span>
  [dump, targets] = max(data.targets); <span class="comment">% targets is integer: the target class, plus 1.</span>
  ret = mean(double(choices ~= targets));
<span class="keyword">end</span>
</pre><pre class="codeoutput">Error using a3 (line 7)
Not enough input arguments.
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br></p></div><!--
##### SOURCE BEGIN #####
% This version of the starter code was published on Tuesday November 8, 01:37 UTC. It is an improved version of the one that was published on November 6, 13:37 UTC.

function a3(wd_coefficient, n_hid, n_iters, learning_rate, momentum_multiplier, do_early_stopping, mini_batch_size)
  warning('error', 'Octave:broadcast');
  if exist('page_output_immediately'), page_output_immediately(1); end
  more off;
  model = initial_model(n_hid);
  from_data_file = load('data.mat');
  datas = from_data_file.data;
  n_training_cases = size(datas.training.inputs, 2);
  if n_iters ~= 0,
    fprintf('Now testing the gradient on the whole training set... ');
    test_gradient(model, datas.training, wd_coefficient);
  end

  % optimization
  theta = model_to_theta(model);
  momentum_speed = theta * 0;
  training_data_losses = [];
  validation_data_losses = [];
  if do_early_stopping,
    best_so_far.theta = -1; % this will be overwritten soon
    best_so_far.validation_loss = inf;
    best_so_far.after_n_iters = -1;
  end
  for optimization_iteration_i = 1:n_iters,
    model = theta_to_model(theta);
    
    training_batch_start = mod((optimization_iteration_i-1) * mini_batch_size, n_training_cases)+1;
    training_batch.inputs = datas.training.inputs(:, training_batch_start : training_batch_start + mini_batch_size - 1);
    training_batch.targets = datas.training.targets(:, training_batch_start : training_batch_start + mini_batch_size - 1);
    gradient = model_to_theta(d_loss_by_d_model(model, training_batch, wd_coefficient));
    momentum_speed = momentum_speed * momentum_multiplier - gradient;
    theta = theta + momentum_speed * learning_rate;

    model = theta_to_model(theta);
    training_data_losses = [training_data_losses, loss(model, datas.training, wd_coefficient)];
    validation_data_losses = [validation_data_losses, loss(model, datas.validation, wd_coefficient)];
    if do_early_stopping && validation_data_losses(end) < best_so_far.validation_loss,
      best_so_far.theta = theta; % this will be overwritten soon
      best_so_far.validation_loss = validation_data_losses(end);
      best_so_far.after_n_iters = optimization_iteration_i;
    end
    if mod(optimization_iteration_i, round(n_iters/10)) == 0,
      fprintf('After %d optimization iterations, training data loss is %f, and validation data loss is %f\n', optimization_iteration_i, training_data_losses(end), validation_data_losses(end));
    end
    if optimization_iteration_i == n_iters, % check gradient again, this time with more typical parameters and with a different data size
      fprintf('Now testing the gradient on just a mini-batch instead of the whole training set... ');
      test_gradient(model, training_batch, wd_coefficient);
    end 
  end
  if do_early_stopping,
    fprintf('Early stopping: validation loss was lowest after %d iterations. We chose the model that we had then.\n', best_so_far.after_n_iters);
    theta = best_so_far.theta;
  end
  % the optimization is finished. Now do some reporting.
  model = theta_to_model(theta);
  if n_iters ~= 0,
    clf;
    hold on;
    plot(training_data_losses, 'b');
    plot(validation_data_losses, 'r');
    legend('training', 'validation');
    ylabel('loss');
    xlabel('iteration number');
    hold off;
  end
  datas2 = {datas.training, datas.validation, datas.test};
  data_names = {'training', 'validation', 'test'};
  for data_i = 1:3,
    data = datas2{data_i};
    data_name = data_names{data_i};
    fprintf('\nThe loss on the %s data is %f\n', data_name, loss(model, data, wd_coefficient));
    if wd_coefficient~=0,
      fprintf('The classification loss (i.e. without weight decay) on the %s data is %f\n', data_name, loss(model, data, 0));
    end
    fprintf('The classification error rate on the %s data is %f\n', data_name, classification_performance(model, data));
  end
end

function test_gradient(model, data, wd_coefficient)
  base_theta = model_to_theta(model);
  h = 1e-2;
  correctness_threshold = 1e-5;
  analytic_gradient_struct = d_loss_by_d_model(model, data, wd_coefficient);
  if size(fieldnames(analytic_gradient_struct), 1) ~= 2,
     error('The object returned by function d_loss_by_d_model should have exactly two field names: .input_to_hid and .hid_to_class');
  end
  if any(size(analytic_gradient_struct.input_to_hid) ~= size(model.input_to_hid)),
     error(sprintf(['The size of .input_to_hid of the return value of d_loss_by_d_model (currently [%d, %d]) should be same as the size of model.input_to_hid (currently [%d, %d])'], size(analytic_gradient_struct.input_to_hid), size(model.input_to_hid)));
  end
  if any(size(analytic_gradient_struct.hid_to_class) ~= size(model.hid_to_class)),
     error(sprintf(['The size of .hid_to_class of the return value of d_loss_by_d_model (currently [%d, %d]) should be same as the size of model.hid_to_class (currently [%d, %d])'], size(analytic_gradient_struct.hid_to_class), size(model.hid_to_class)));
  end
  analytic_gradient = model_to_theta(analytic_gradient_struct);
  if any(isnan(analytic_gradient)) || any(isinf(analytic_gradient)),
     error('Your gradient computation produced a NaN or infinity. That is an error.')
  end
  % We want to test the gradient not for every element of theta, because that's a lot of work. Instead, we test for only a few elements. If there's an error, this is probably enough to find that error.
  % We want to first test the hid_to_class gradient, because that's most likely to be correct (it's the easier one).
  % Let's build a list of theta indices to check. We'll check 20 elements of hid_to_class, and 80 elements of input_to_hid (it's bigger than hid_to_class).
  input_to_hid_theta_size = prod(size(model.input_to_hid));
  hid_to_class_theta_size = prod(size(model.hid_to_class));
  big_prime = 1299721; % 1299721 is prime and thus ensures a somewhat random-like selection of indices.
  hid_to_class_indices_to_check = mod(big_prime * (1:20), hid_to_class_theta_size) + 1 + input_to_hid_theta_size;
  input_to_hid_indices_to_check = mod(big_prime * (1:80), input_to_hid_theta_size) + 1;
  indices_to_check = [hid_to_class_indices_to_check, input_to_hid_indices_to_check];
  for i = 1:100,
    test_index = indices_to_check(i);
    analytic_here = analytic_gradient(test_index);
    theta_step = base_theta * 0;
    theta_step(test_index) = h;
    contribution_distances = [-4:-1, 1:4];
    contribution_weights = [1/280, -4/105, 1/5, -4/5, 4/5, -1/5, 4/105, -1/280];
    temp = 0;
    for contribution_index = 1:8,
      temp = temp + loss(theta_to_model(base_theta + theta_step * contribution_distances(contribution_index)), data, wd_coefficient) * contribution_weights(contribution_index);
    end
    fd_here = temp / h;
    diff = abs(analytic_here - fd_here);
    % fprintf('%d %e %e %e %e\n', test_index, base_theta(test_index), diff, fd_here, analytic_here);
    if (diff > correctness_threshold) && (diff / (abs(analytic_here) + abs(fd_here)) > correctness_threshold),
      part_names = {'input_to_hid', 'hid_to_class'};
      error(sprintf('Theta element #%d (part of %s), with value %e, has finite difference gradient %e but analytic gradient %e. That looks like an error.\n', test_index, part_names{(i<=20)+1}, base_theta(test_index), fd_here, analytic_here));
    end
    if i==20, fprintf('Gradient test passed for hid_to_class. '); end
    if i==100, fprintf('Gradient test passed for input_to_hid. '); end
  end
  fprintf('Gradient test passed. That means that the gradient that your code computed is within 0.001%% of the gradient that the finite difference approximation computed, so the gradient calculation procedure is probably correct (not certainly, but probably).\n');
end

function ret = logistic(input)
  ret = 1 ./ (1 + exp(-input));
end

function ret = log_sum_exp_over_rows(a)
  % This computes log(sum(exp(a), 1)) in a numerically stable way
  maxs_small = max(a, [], 1);
  maxs_big = repmat(maxs_small, [size(a, 1), 1]);
  ret = log(sum(exp(a - maxs_big), 1)) + maxs_small;%+log(exp(maxs_small))
  %=log( sum(exp(a-maxs_big),1).*exp(maxs_small) )
  %=log(sum(exp(a),1)./exp(max_small).*exp(max_small))
end

function ret = loss(model, data, wd_coefficient)
  % model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>.
  %It contains the weights from the input units to the hidden units.
  % model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>. 
  %It contains the weights from the hidden units to the softmax units.
  % data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>.
  %Each column describes a different data case. 
  % data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>. 
  %Each column describes a different data case. It contains a one-of-N encoding of the class, i.e. one element in every column is 1 and the others are 0.
	 
  % Before we can calculate the loss, we need to calculate a variety of intermediate values, 
  %like the state of the hidden units. This is the
  % forward pass, and you'll likely want to copy it into d_loss_by_d_model, 
  %because these values are also very useful for that function.
  hid_input = model.input_to_hid * data.inputs; % input to the hidden units, i.e. before the logistic. size: <number of hidden units> by <number of data cases>
  hid_output = logistic(hid_input); % output of the hidden units, i.e. after the logistic. size: <number of hidden units> by <number of data cases>
  class_input = model.hid_to_class * hid_output; % input to the components of the softmax. size: <number of classes, i.e. 10> by <number of data cases>
    
  % The following three lines of code implement the softmax.
  % However, it's written differently from what the lectures say.
  % In the lectures, a softmax is described using an exponential divided by a sum of exponentials.
  % What we do here is exactly equivalent (you can check the math or just check it in practice), but this is more numerically stable. 
  % "Numerically stable" means that this way, there will never be really big numbers involved.
  % The exponential in the lectures can lead to really big numbers, which are fine in mathematical equations, but can lead to all sorts of problems in Octave.
  % Octave isn't well prepared to deal with really large numbers, like the number 10 to the power 1000. Computations with such numbers get unstable, so we avoid them.
  class_normalizer = log_sum_exp_over_rows(class_input); 
  % log(sum(exp of class_input)) is what we subtract to get properly normalized log class probabilities. 
  %size: <1> by <number of data cases>
  log_class_prob = class_input - repmat(class_normalizer, [size(class_input, 1), 1]); 
  % log of probability of each class. size: <number of classes, i.e. 10> by <number of data cases>
  class_prob = exp(log_class_prob); 
  % probability of each class. Each column (i.e. each case) sums to 1. size: <number of classes, i.e. 10> by <number of data cases>
  
  classification_loss = -mean(sum(log_class_prob .* data.targets, 1));
  % select the right log class probability using that sum; then take the mean over all data cases.
  wd_loss = sum(model_to_theta(model).^2)/2*wd_coefficient; 
  % weight decay loss. very straightforward: E = 1/2 * wd_coeffecient * theta^2
  ret = classification_loss + wd_loss;
end

function ret = d_loss_by_d_model(model, data, wd_coefficient)
  % model.input_to_hid is a matrix of size <number of hidden units> by <number of inputs i.e. 256>
  % model.hid_to_class is a matrix of size <number of classes i.e. 10> by <number of hidden units>
  % data.inputs is a matrix of size <number of inputs i.e. 256> by <number of data cases>
  % data.targets is a matrix of size <number of classes i.e. 10> by <number of data cases>

  % The returned object is supposed to be exactly like parameter <model>, i.e. it has fields ret.input_to_hid and ret.hid_to_class. However, the contents of those matrices are gradients (d loss by d model parameter), instead of model parameters.
	 
  % This is the only function that you're expected to change. Right now, it just returns a lot of zeros, which is obviously not the correct output. Your job is to change that.
  ret.input_to_hid = model.input_to_hid * 0;
  ret.hid_to_class = model.hid_to_class * 0;
end

function ret = model_to_theta(model)
  % This function takes a model (or gradient in model form), and turns it into one long vector. See also theta_to_model.
  input_to_hid_transpose = transpose(model.input_to_hid);
  hid_to_class_transpose = transpose(model.hid_to_class);
  ret = [input_to_hid_transpose(:); hid_to_class_transpose(:)];
end

function ret = theta_to_model(theta)
  % This function takes a model (or gradient) in the form of one long vector (maybe produced by model_to_theta), and restores it to the structure format, i.e. with fields .input_to_hid and .hid_to_class, both matrices.
  n_hid = size(theta, 1) / (256+10);
  ret.input_to_hid = transpose(reshape(theta(1: 256*n_hid), 256, n_hid));
  ret.hid_to_class = reshape(theta(256 * n_hid + 1 : size(theta,1)), n_hid, 10).';
end

function ret = initial_model(n_hid)
  n_params = (256+10) * n_hid;
  as_row_vector = cos(0:(n_params-1));
  ret = theta_to_model(as_row_vector(:) * 0.1); % We don't use random initialization, for this assignment. This way, everybody will get the same results.
end

function ret = classification_performance(model, data)
  % This returns the fraction of data cases that is incorrectly classified by the model.
  hid_input = model.input_to_hid * data.inputs; % input to the hidden units, i.e. before the logistic. size: <number of hidden units> by <number of data cases>
  hid_output = logistic(hid_input); % output of the hidden units, i.e. after the logistic. size: <number of hidden units> by <number of data cases>
  class_input = model.hid_to_class * hid_output; % input to the components of the softmax. size: <number of classes, i.e. 10> by <number of data cases>
  
  [dump, choices] = max(class_input); % choices is integer: the chosen class, plus 1.
  [dump, targets] = max(data.targets); % targets is integer: the target class, plus 1.
  ret = mean(double(choices ~= targets));
end

##### SOURCE END #####
--></body></html>